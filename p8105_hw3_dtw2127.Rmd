---
title: "p8105_hw3_dtw2127"
author: "Dee Wang"
date: "18/10/2021"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(ggplot2)

#knitr::opts_chunk$set(echo = TRUE) 
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

First we'll load in the instacart data and do an exploration of the dataset.

```{r}
data("instacart")

```

The instacart dataset contains `r nrow(instacart)` rows of data for `r ncol(instacart)` variables. Key variables include order ID, product ID, add to cart order, user ID, order hour of day, days since prior order, and department. 

The dataset contains information from `r pull(instacart, user_id) %>% unique() %>% length()` users. 

Let's see how many different products are carried by each department.

```{r}
instacart %>% 
  group_by(department) %>% 
  summarize(n_products = n_distinct(product_name)) %>% 
  arrange(desc(n_products))
```
The snacks department carries the most number of different products, while the bulk department carries the least. 

Let's see which day of the week is most popular for orders.

```{r}
instacart %>% 
  mutate(order_dow = recode(order_dow, `0` = "Sunday", 
                            `1` = "Monday", 
                            `2` = "Tuesday", 
                            `3` = "Wednesday", 
                            `4` = "Thursday", 
                            `5` = "Friday", 
                            `6` = "Saturday")) %>%
  group_by(order_dow) %>%
  summarize(n_orders = n_distinct(order_id)) %>% 
  arrange(desc(n_orders))
  
```
Sundays are the most popular days for orders, followed by Monday. 

There are `r pull(instacart, aisle) %>% unique() %>% length()` aisles. Let's determine which aisle the most items are ordered from. 

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  slice_max(n_obs, n=3) %>%
  knitr::kable()
```
The most items are ordered from the fresh vegetables, fresh fruits, and packaged vegetables and fruits aisle.  

We'll make a plot that shows the number of items ordered from each aisle. 

```{r, fig.width = 10, fig.height = 11}

instacart %>% 
  group_by(aisle, department) %>% 
  summarize(n_obs = n()) %>% filter(n_obs > 10000) %>% 
  ggplot(aes(x = forcats::fct_reorder(aisle, n_obs), y = n_obs)) +
  geom_bar(stat = "identity", width = 0.7, position = "dodge") + 
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "aisle", y = "number of items ordered") + 
  coord_flip()

#arrange bars in ascending order
```
As we showed previously, the most popular aisles are fresh vegetables, fresh fruits and packaged vegetables and fruits. The number of items ordered from fresh vegetables and fresh fruits approximately doubles the items ordered from pacakaged vegetables and fruits. The least number of items are ordered from the dry pasta, oils vinegars and butter aisle. 

Next we'll create a table showing the three most popular items in each of the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits".

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", 
                      "packaged vegetables fruits", 
                      "dog food care")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_obs = n()) %>% 
  slice_max(order_by = n_obs, n = 3) %>% 
  knitr::kable()

```

The most popular items in the baking ingredients aisle are light brown sugar, pure baking soda and cane sugar. The most popular items in the dog food care aisle are snack sticks chicken & rice recipe dog treats, organic chicken & brown rice recipe, and small dog biscuits. The most popular items in the packaged vegetables fruits aisle are organic baby spinach, raspberries and blueberries. Spinach is by far the most popular item of the packaged vegetables and fruits aisle. 

Next we'll make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week and will format the table for human readers. 


```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_dow = recode(order_dow, `0` = "Sunday", 
                            `1` = "Monday", 
                            `2` = "Tuesday", 
                            `3` = "Wednesday", 
                            `4` = "Thursday", 
                            `5` = "Friday", 
                            `6` = "Saturday")) %>%
  mutate(order_dow = factor(order_dow, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour_of_day) %>% 
  knitr::kable(digits = 1)
```
On average, orders for Coffee Ice Cream happen later in the day as compared to orders for Pink Lady Apples. Orders for Coffee Ice Cream are on average placed early to mid afternoon. For all days of the week, orders for Pink Lady Apples are placed on average in the late afternoon or early afternoon. 

## Problem 2

First, let's load in the BRFSS data and then do some data cleaning. 

```{r}
data("brfss_smart2010") #rename some of the variables

brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  mutate(topic = as.factor(topic)) %>%
  filter(topic == "Overall Health") %>%
  mutate(response = as.factor(response)) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very Good", "Excellent")))
```

Let's determine how many states were observed at 7 or more locations in 2002 and 2010. 

```{r}
brfss %>% 
  filter(year %in% c(2002, 2010)) %>% 
  select(locationabbr, geo_location, year) %>% 
  distinct() %>% 
  group_by(locationabbr, year) %>% 
  summarize(n_obs = n()) %>%
  filter(n_obs >= 7) %>% 
  arrange(year)
```
In 2002, CT, FL, MA, NC, NJ and PA were observed at 7 or more locations. 

In 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA were observed at 7 or more locations. 

In 2002, the most number of observations were made in PA (10). In 2010, the most number of observations were made in NJ (19). 

Next, we will construct a dataset limited to 'Excellent' responses and containing year, state and a variable averaging data_value across locations within a state, and we'll make a spaghetti plot of the average value over time.

```{r}
brfss %>%
  filter(response == "Excellent") %>% 
  group_by(locationabbr, year) %>% 
  mutate(mean_data_value = mean(data_value)) %>% 
  select(year, locationabbr, mean_data_value) %>% 
  ggplot(aes(x = year, y = mean_data_value, color = locationabbr)) + 
  geom_line(aes(group = locationabbr)) +
  theme(legend.position = "right")
  
```

The range of the mean data values is between 10 and 30. There is a general decreasing trend in mean data values as the year increases. There are notable dips in mean data values in 2005 and 2009 in WV and 2009 in IN. 

Next, we'll make a two-panel plot showing the distribution of data_value for responses ("Poor" to "Excellent") for 2006 and 2010 among locations in NY State.

```{r}
brfss %>% 
  filter(response %in% c("Poor", "Fair", "Good",
                         "Very Good", "Excellent"),
         year %in% c(2006,2010)) %>% 
  group_by(locationabbr, geo_location) %>% 
  ggplot(aes(x = data_value, fill = response)) + 
  geom_density(alpha = .4, adjust = .5, color = "blue") + 
  facet_grid(. ~ year)
```
The distribution of data values for the different responses was quite similar in 2006 and 2010. The distribution of data values for those responding with "Poor" was narrowest and widest for those responding "Excellent". The data values were lowest for those who responded "Poor" and highest for those who responded "Good". 

## Problem 3 

Let's load in and tidy the data. 

```{r}
accel_data = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity", 
    names_prefix = "activity_",
    values_to = "activity_count") %>% 
  mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"), 1, 0), 
         activity = as.numeric(activity), 
         weekend = as.factor(weekend)) %>% 
  rename(minute_of_day = activity)

```

The resulting dataset has the variables week, day_id, day, activity, activity_count and weekend. For these 6 variables, there are `nrow(accel_data)` observations over ` r pull(accel_data, day_id) %>% unique() %>% length()` days. The minimum activity_count is `r pull(accel_data, activity_count) %>% min()` and the max activity count is `r pull(accel_data, activity_count) %>% max()`. 

We'll aggregate minutes of activity over each day and create a table with the totals. 

```{r}
accel_data %>% 
  group_by(day_id) %>% 
  summarize(activity_total = sum(activity_count)) %>% 
  knitr::kable()
```

There are no immediate trends apparent. Activity appears to be quite uniform over the 35 day period, with the activity total tending to stay within the 30,000 - 50,000 count range. The low total activity count is 1440, and high 685910. 

Finally, we will create a plot showing 24 hour activity time courses for each day. 

```{r}
accel_data %>% 
  ggplot(aes(x = minute_of_day, y = activity_count)) + 
  geom_point(aes(color = day), size = .3, alpha = 0.4) + 
  scale_colour_manual(values = rainbow(7)) 
```

Activity count is lower at the beginning of the day and picks up at around the 300 minute mark. There are periods of high activity in the middle of the day on Saturday and Sunday. On Mondays, Wednesday and Fridays, there is a burst in activity near the end of the day. 